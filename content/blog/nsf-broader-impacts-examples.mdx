---
title: 'NSF Broader Impacts: 15 Examples That Impressed Review Panels'
description: 'Fifteen specific NSF broader impacts examples organized by category, with details on why reviewers scored them highly and how to avoid common mistakes.'
date: '2025-11-08'
author: 'Ana Estrada'
---

![Cover image](//904133d31006c5cfee432029d4ab4b31.cdn.bubble.io/f1682667928608x297900224056478500/getty_525525661_111799.jpg)

Broader impacts is the section where NSF proposals go to die. Not because the science is weak, but because PIs treat broader impacts as a box to check rather than a genuine dimension of their work. Reviewers see this instantly. A proposal with outstanding intellectual merit and perfunctory broader impacts will not score as well as a proposal with strong merit and genuinely integrated broader impacts. NSF review panels evaluate both criteria independently, and a weakness in either one drags down the overall assessment.

The challenge is that "broader impacts" is intentionally broad. NSF defines it as the potential to benefit society and contribute to the achievement of specific, desired societal outcomes. That definition encompasses everything from K-12 education to national security to economic competitiveness. The openness is by design -- it allows researchers in every field to find authentic connections between their work and public benefit. But that same openness leaves many PIs uncertain about what actually impresses review panels.

This guide provides 15 specific broader impacts examples, organized by category, that review panels have scored highly. For each example, I describe the activity, the target audience, how it was integrated with the research, and why it worked. These are composite examples drawn from real proposals, not reproductions of any single submission.

## Intellectual Merit vs. Broader Impacts: A Necessary Distinction

Before the examples, a clarification that matters for how you structure your proposal.

**Intellectual merit** concerns the potential to advance knowledge within and across fields. It is evaluated based on the research question, the approach, the PI's qualifications, and the resources available.

**Broader impacts** concern the potential to benefit society or contribute to desired societal outcomes. NSF identifies several categories: full participation of underrepresented groups, improved STEM education, increased public scientific literacy, enhanced infrastructure for research and education, and societal benefit.

These two criteria are evaluated separately in panel review. A panelist may rate your intellectual merit as "Excellent" and your broader impacts as "Good," and the overall rating reflects both. Proposals that receive "Excellent" on both criteria are funded at much higher rates than those with a split rating.

The most effective proposals blur the line between merit and impacts -- not by conflating them, but by designing research programs where the broader impacts flow naturally from the scientific work. That integration is what distinguishes the examples below from the generic outreach paragraphs that reviewers dismiss.

## Category 1: K-12 Education and STEM Pipeline

### Example 1: Sensor Design Workshop Series for Rural High Schools

**The activity:** A materials science PI designed a six-session workshop series in which rural high school students built low-cost environmental sensors using Arduino microcontrollers and the same polymer materials being studied in the research. Students collected water quality data from local streams, analyzed it, and presented findings at a regional science fair.

**The target audience:** Students from three rural school districts with no AP science courses and limited laboratory equipment. Approximately 90 students per year across two years.

**Integration with research:** The sensors used simplified versions of the polymer membranes being developed in the lab. Students were not just building generic electronics -- they were working with the same material chemistry that formed the basis of the NSF-funded research. Data from student-collected samples supplemented the lab's environmental testing dataset.

**Why reviewers scored it highly:** The activity was specific (named schools, defined timeline, measurable participation numbers), directly connected to the research (same materials and methods), addressed an underserved population (rural students with limited STEM access), and produced tangible outcomes (student presentations, supplementary data for the research program). It was not a vague promise to "develop educational materials."

### Example 2: Graduate Student-Led Science Modules for Title I Elementary Schools

**The activity:** A biology PI created a structured program where three graduate students per year developed and delivered hands-on science modules in four Title I elementary schools. Each module connected to the PI's research on pollinator ecology. Graduate students received training in science communication and pedagogy before entering classrooms.

**The target audience:** Third through fifth graders in schools where over 80% of students qualified for free or reduced lunch. Approximately 400 students per year.

**Integration with research:** The modules taught concepts directly related to the research -- pollinator identification, habitat assessment, and the relationship between flower diversity and pollinator health. Students planted pollinator gardens at their schools, and the PI's lab monitored these gardens as part of the broader ecological survey.

**Why reviewers scored it highly:** The program served a specific underserved population, trained graduate students in communication skills they would carry throughout their careers, and generated data relevant to the research. The pollinator gardens created a lasting physical legacy beyond the grant period. The reviewers noted that the program was designed to be self-sustaining through institutional partnerships with the school district.

### Example 3: Summer Research Immersion for Community College Students

**The activity:** A computer science PI hosted four community college students each summer for an eight-week paid research experience in the lab. Students worked on real research tasks (annotating training datasets, testing algorithms, writing documentation) and presented their work at the university's undergraduate research symposium.

**The target audience:** Students from community colleges with no research opportunities, with recruitment prioritizing first-generation college students and underrepresented minorities in computing.

**Integration with research:** Students contributed to the actual research project, not a simulated exercise. Their data annotation work produced labeled datasets used in published papers. Two students over the grant period were co-authors on conference publications.

**Why reviewers scored it highly:** Paid research experience for community college students addresses a critical gap in the STEM pipeline. The activity produced real research contributions (co-authored papers) rather than superficial exposure. The PI included a mentoring plan and tracked outcomes (transfer rates, enrollment in four-year programs, continued research participation).

## Category 2: Broadening Participation

### Example 4: Research Mentoring Network for Women in Geosciences

**The activity:** A geoscience PI established a mentoring network pairing early-career women researchers (graduate students and postdocs) with mid-career women faculty across five institutions. The network included quarterly virtual seminars, annual in-person retreats, and structured one-on-one mentoring relationships.

**The target audience:** 30 to 40 women researchers at various career stages, with emphasis on institutions where women were significantly underrepresented in geoscience departments.

**Integration with research:** The seminars focused on research topics directly related to the funded project (climate modeling), and several mentoring pairs developed collaborative research projects that extended the scope of the original proposal. The network also served as a recruitment pipeline for the PI's research team.

**Why reviewers scored it highly:** The program addressed a documented disparity (women's underrepresentation in geosciences) with a structured, evidence-based approach (mentoring networks are supported by retention research). The cross-institutional design amplified impact beyond the PI's home department. The activity was sustainable because the mentoring relationships, once established, continued without ongoing grant funding.

### Example 5: Accessible Laboratory Protocols for Researchers with Disabilities

**The activity:** A chemistry PI developed modified laboratory protocols and adaptive equipment configurations that enabled researchers with visual impairments and mobility limitations to perform the synthetic chemistry procedures used in the research. The protocols were published as open-access supplementary materials and shared through the AccessSTEM network.

**The target audience:** Researchers with disabilities in chemistry and related fields, plus laboratory managers seeking to create more accessible research environments.

**Integration with research:** The modified protocols were developed as a direct consequence of accommodating a graduate student with a visual impairment in the PI's lab. The adaptations led to improved safety protocols for all lab members and, in two cases, to methodological improvements that increased experimental reproducibility.

**Why reviewers scored it highly:** This is an example of a broader impact that emerged organically from the research environment rather than being artificially attached. The outcome -- open-access accessible protocols -- had lasting value for the broader scientific community. The fact that the adaptations actually improved research methodology demonstrated that accessibility and scientific quality are not in tension.

### Example 6: Bridge Program for Veterans Transitioning to STEM Careers

**The activity:** An engineering PI partnered with a veterans' services organization to create a 10-week paid laboratory rotation for recently separated military veterans interested in STEM graduate education. The program included GRE preparation and mentorship from veteran graduate students.

**Target audience:** 8 to 10 Post-9/11 veterans per year.

**Integration with research:** Veterans brought applicable technical skills (materials testing, structural analysis) and contributed to destructive testing experiments, producing data used in publications. Pilot year tracking showed 60% of participants subsequently enrolled in STEM graduate programs.

## Category 3: Public Engagement and Scientific Literacy

### Example 7: Interactive Museum Exhibit on Ocean Acidification

**The activity:** An oceanography PI collaborated with a regional science museum to develop a permanent interactive exhibit on ocean acidification. The exhibit included hands-on demonstrations of pH measurement, a real-time data display connected to the PI's ocean monitoring buoys, and a touchscreen simulation allowing visitors to model the effects of different emission scenarios on marine ecosystems.

**The target audience:** General public visitors to the museum, approximately 150,000 per year, with a significant proportion of school field trip groups.

**Integration with research:** The exhibit displayed real data from the PI's research instrumentation in near-real-time. Visitors could see actual pH measurements from the ocean monitoring stations, making the research tangible and immediate. The exhibit was updated as new research findings were published.

**Why reviewers scored it highly:** The scale of impact (150,000 visitors per year) was substantial and credible because it leveraged existing museum infrastructure. The real-time data connection was a genuine link to the research, not a tangential relationship. The exhibit was permanent, meaning the broader impact would persist long after the grant ended.

### Example 8: Podcast Series Translating Computational Research for Policy Audiences

**The activity:** A data science PI produced a 12-episode podcast series translating research findings for policymakers and journalists. Each episode featured the PI in conversation with a policy expert discussing how specific research outputs (published papers, new datasets) informed questions about urban planning, public health resource allocation, and transportation optimization. The podcast reached approximately 8,000 downloads per episode.

**Why it worked:** Each episode corresponded to an actual research output, not generic science communication. The conversations with policy experts sometimes identified new research questions, demonstrating genuine bidirectional engagement between research and practice.

## Category 4: Infrastructure for Research and Education

### Example 9: Open-Source Software Package for Field Data Collection

**The activity:** An ecology PI developed and released an open-source mobile application for field data collection that standardized biodiversity observation protocols. The app included GPS tagging, photo integration, taxonomic identification support, and automated data formatting for submission to major ecological databases.

**The target audience:** Field ecologists, citizen scientists, and undergraduate research programs at primarily undergraduate institutions (PUIs).

**Integration with research:** The app was developed to support the PI's own field research program but was designed from the outset for general use. The standardized protocols embedded in the app reflected the methodological standards established by the PI's research group.

**Why reviewers scored it highly:** The software addressed a real methodological challenge in the field (inconsistent data collection protocols). Making it open-source and freely available maximized its impact. The fact that it was designed for use by citizen scientists and PUI undergraduates expanded its reach well beyond the PI's own institution.

### Example 10: Shared Instrumentation and Training Program for Regional Institutions

**The activity:** A physics PI allocated 15% of a major instrument's time to researchers from four regional institutions lacking equivalent equipment, plus an annual training workshop and graduate student exchanges. Target: faculty and students at predominantly teaching institutions and minority-serving institutions.

**Why it worked:** Partner institutions' projects generated complementary data that enriched the PI's research, producing three collaborative publications. The structured training ensured that access translated into actual research capacity rather than token instrument time.

### Example 11: Curated Open Dataset with Educational Documentation

**The activity:** A social science PI released a large-scale, de-identified dataset as an open educational resource with analysis tutorials and sample assignments for methods courses. Graduate students wrote the documentation as a training exercise in data communication and reproducibility.

**Why it worked:** The activity served both research infrastructure (enabling secondary analyses) and education (real-world data for methods courses). Open data release is increasingly valued by NSF, and packaging it with educational materials amplified utility beyond what a raw data deposit would achieve.

## Category 5: Societal Benefit

### Example 12: Decision Support Tool for Municipal Water Managers

**The activity:** An environmental engineering PI developed a web-based decision support tool that helped municipal water managers assess the vulnerability of their water systems to emerging contaminants. The tool incorporated models from the research and allowed managers to input their system parameters and receive risk assessments.

**The target audience:** Water utility managers serving communities of 10,000 to 100,000 people, many of which lacked the engineering staff to conduct independent vulnerability assessments.

**Integration with research:** The tool was a direct application of the contamination transport models developed in the research. Every model improvement during the grant period was incorporated into the tool, creating a living connection between the research and its public application.

**Why reviewers scored it highly:** The activity addressed a concrete societal need (water safety for small communities), targeted users who would genuinely benefit (under-resourced water utilities), and was directly derived from the research rather than tangentially related. The PI included letters of support from three water utility managers who had beta-tested the tool.

### Example 13: Wildfire Risk Communication Partnership with Tribal Nations

**The activity:** A climate science PI partnered with two tribal nations to co-develop bilingual wildfire risk communication materials integrating Western climate science with traditional ecological knowledge.

**Why it worked:** The partnership was genuinely reciprocal -- the tribal communities contributed oral historical fire records that improved model calibration, and the science produced culturally appropriate risk materials. Tribal leadership was involved in project design from the outset, and bilingual materials demonstrated respect for the communities being served.

### Example 14: Assistive Technology Prototypes for Aging Populations

**The activity:** A mechanical engineering PI co-designed assistive devices (adaptive grip tools, low-force door handles) with a local senior center, applying compliant mechanism principles from the funded research.

**Why it worked:** The co-design process produced tangible prototypes with direct benefits, and the real-world human factors constraints pushed the research in productive new directions that laboratory studies alone would not have identified. Community members were partners, not passive recipients.

### Example 15: Climate Data Dashboard for Coastal Communities

**The activity:** An atmospheric science PI developed a publicly accessible dashboard visualizing sea-level rise projections, storm surge models, and flood risk for 12 coastal communities. The dashboard was designed in partnership with municipal planning departments.

**Why it worked:** The dashboard visualized outputs from the funded climate models, updated as model resolution improved. The partnership with municipal planners ensured actual utility, and the PI documented planning decisions informed by the dashboard data -- tangible evidence of societal impact.

## How to Avoid the "Tacked On" Problem

The single most common criticism is that broader impacts feel disconnected from the research. The principles that prevent this: start with the research (ask what naturally connects to public benefit, not what outreach to add), be specific (name schools, organizations, participant numbers, timelines), include feasibility evidence (letters from partner organizations), budget for the activities (broader impacts that appear nowhere in the budget are not credible), measure outcomes (pre/post surveys, download counts, participation tracking), and build on what you already do (extensions of existing activities are more credible than brand-new initiatives).

The proposals that score highest are the ones where reviewers cannot easily separate the research from its societal connections -- where broader impacts feel like a natural extension of what the PI is already doing, amplified by NSF funding.

## Keep Reading

- [Addressing Broader Impacts: A Guide to Enhancing Your NSF Grant Proposal](/blog/addressing-broader-impacts-a-guide-to-enhancing-your-nsf-grant-proposal-s-appea)
- [NSF Grant Writing Guide: From Concept to Submission](/blog/nsf-grant-writing-guide-concept-to-submission)
- [What NSF Reviewers Wish You Knew](/blog/what-nsf-reviewers-wish-you-knew)
- [Granted AI Features](/features)

---

**Ready to write your next proposal?** [Granted AI](https://grantedai.com/pricing) analyzes your RFP, coaches you through the requirements, and drafts every section. Start your [7-day free trial](https://grantedai.com/pricing) today.
